As already mentioned before, various applications for the bootstrap method exist. For example, early clinical trials usually suffer from small sample sizes. For example, consider a very early stage for a new drug. Since it's effects are only considered theoretically, ethical standards would forbid testing such a drug on a large sample of patients. On the other hand, small samples are not reliable when it comes to statistical inference. However, valid inference is necessary to judge about approving or declining the public usage of this treatment. The solution lies in bootstrapping a small sample. For example, the clinical trial consists of 16 patients receiving the new treatment or the placebo, this leads to a two-sample of matched pairs design. Here, bootstrap solves this small sample problem and leads to reliable results, see \citet{boot4} and \citet{boot2}.\\
\\
\noindent In finance, especially portfolio management, parametric bootstrap and resampling in general can provide large samples when analytic solutions are not possible. Consider the following minimization problem (Condition Value-at-Risk):
\begin{align}
	\min_{x} \quad - \frac{1}{1 - \alpha}\int_{x^{\intercal}\mu \leq -VaR_{\alpha}(x)} x^{\intercal}\mu f\left(x^{\intercal}\mu \mid x\right)\dd x^{\intercal}\mu.
\end{align}
This has no analytic solutions. However, a portfolio manager wants to obtain the portfolio weights $x$. \citet{erin} solve this problem by sampling a vector of weights $x$ from the uniform distribution $n_{b}$ times and compute $n_{b}$ (empirical) CVaRs and choose the vector $x$ of weights which corresponds to the smallest CVaR of all.\\
\\
\noindent Bagging, as mentioned and described in \citet{stat1}, uses bootstrapping to improve Machine Learning techniques. By bootstrapping and its property of drawing with replacement, one creates $n_b$ samples on which an algorithm can be trained and estimated. The $n_b$ models are then aggregated into one (for example by majority vote), see \citet{rf2}. This parallel design is distinct to sequential designs such as Boosting. Random Forests are a special case of this, since it grows $n_b$ trees instead of only one and thus significantly reduces the variance of the tree model, see \citet{rf1} and \citet{rf3}.\\
\\
\noindent Generally, bootstrapping can be applied to many fields of statistical and applied sciences. 