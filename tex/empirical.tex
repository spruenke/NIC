This section covers the empirical results using the methods mentioned earlier. Our interest lies in the aspects of accuracy, computation time and complexity, and for tests the errors of type I and II. 
\subsection{Accuracy}
The first point of interest is whether bootstrap techniques reach an acceptable level of accuracy. In this subsection we will asses this with the estimation of summary statistics (quantiles, mean and standard deviation). However, such an level of accuracy can be generalized easily without major drawbacks. Furthermore, we will adress the computation time of the bootstrap with respect to sample size and bootstrap iterations. However, the computation time is not of much interest in such a setting and will be discussed more deeply in the following subsection. \\
As already mentioned, we measure accuracy with typical MSE and MAE. Figure ~\ref{fig:summary} shows how MAE and MSE behave in estimating the mean of a sample using resampling methods. A complete table for summary statistics estimation can be found in Appendix A, see table~\ref{tab:summary}. 

\begin{figure}\label{fig:summary}
	\includegraphics[scale=0.3]{./figures/plot_summary.pdf}
	\caption[Summary MSE and MAE]{Red Line: Nonparametric Bootstrap, Blue Line: Wild Bootstrap. Each row corresponds to a different number of sample size from ($5, 10, 50, 100, 200$), the first two columns correspond to the R-Version, whereas the third and fourth to the C++ version. The last column represents the median computation time (with red being the R and blue the C++ algorithm).}
\end{figure}

\subsection{Computational Complexity}

In this subsection we want to adress one of the main advantages of C/C++. This is the speed of computations. Whereas R (or other languages such as python) are very high level languages which allow the user to do scientific analyses easily, lower level languages such as C and C++ offer a great advantage in speed for the cost of intuitiveness. To demonstrate such issues we present a simulated linear regression problem as mentioned in section~\ref{sec:sim}. In the following, we also show a figure of accuracy for the coefficients, although we used a different accuracy measure, namely the compound MSE and compound MAE as mentioned in the same section~\ref{sec:sim}. Since the accuracy is related to the accuracy of summary statistics and also dependent on the accuracy of the specific algorithm itself, there is not much additional error due to bootstrap (no systematic bias), see table~\ref{tab:regError} in Appendix B. 

\begin{figure}\label{fig:regErr}
	\includegraphics[scale=0.3]{./figures/plot_regression_error.pdf}
	\caption[Boxplot Coefficients]{Boxplot for all coefficients with respect to the bootstrap technique used}
\end{figure}

We see, that the estimated coefficients are quite close to the true coefficients. Also, we see that wild bootstrap provides slightly better results than nonparametrixc bootstrap. Since the origin of wild bootstrap is in regression analysis, this is not much surprising. However, the following plot shows how R and C++ differ in terms of computation time.

\begin{figure}\label{fig:regErr}
	\includegraphics[scale=0.3]{./figures/plot_regression.pdf}
	\caption[Regression Complexity]{Median speed of linear regression: Blue Line: C++, Red Line: R. Each row corresponds to a different sample size from($5, 10, 50, 100, 200$).}
\end{figure}

Although the scale is already in log(microseconds) there is a significant difference between those two. The data used for the plot is the median time for each function, for full summary statistics of the time see table~\ref{reg:time}. For both languages the time necessary increases with increased bootstrap iterations, which is only natural. However, even for $10000$ bootstrap iterations C++ is roughly eight times faster than it's R counterpart. Since the simulated problem is of quite simple nature this highlights a huge advantage of lower level languages. We expect that even for more complex linear regression problems (more variables) the computation time increases and that problems which are more complex in their nature themselves would have an even higher advantage in using low level languages (such as generalized linear models from exponential families, classifier problems, dimension reduction techniques) and thus recommend more research in that area.

\subsection{Type I and II Errors}
Finally, we want to demonstrate the utility of bootstrap in a specific area: statistical tests. We do this exemplary with the t-Test (one and two samples), although we do not show how permutation tests compare to these, which work especially well for two-sample problems. It is noteworthy, that resampling is an important aspect in the field of statistical tests since there exist a large number of research fields where such tests are necessary but suffer from small sample sizes (e.g. translational studies in clinical research). Ideally, a resampled test should reach a Type-I-Error equal to the significance level and also maintain a high power, just as we expect this from a statistical test under large sample situations (or at least asymptotically in theory). Thus, we show Type-I and power for one- and two-sample t-Tests with respect to different sample sizes and different bootstrap iterations. 

\begin{figure}\label{fig:t1_s1}
	\includegraphics[scale=0.3]{./figures/plot_t1s1.pdf}
	\caption[Type-I-Error One Sample]{Type-1-Error for four different populations in a one-sample setting: Green: , Red:, Blue:, Black:. Each row corresponds to a different sample size from($5, 10, 50, 100, 200$). The first two columns correspond to the nonparametric bootstrap in R, C++, whereas the third and fourth column correspond to the wild bootstrap in R, C++. The last column depicts the median computation time.}
\end{figure}


\begin{figure}\label{fig:t1_s2}
	\includegraphics[scale=0.3]{./figures/plot_t1s2.pdf}
	\caption[Type-I-Error Two Sample]{Type-1-Error for four different populations in a two-sample setting: Green: , Red:, Blue:, Black:. Each row corresponds to a different sample size from($5, 10, 50, 100, 200$). The first two columns correspond to the nonparametric bootstrap in R, C++, whereas the third and fourth column correspond to the wild bootstrap in R, C++. The last column depicts the median computation time.}
\end{figure}